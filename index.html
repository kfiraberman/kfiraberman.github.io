
<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link type="text/css" rel="stylesheet" href="style.css"/>
	<title>Kfir Aberman</title>
</head>
<body>

<!-- menu  -->
<div id=menu>

<p><a href=#about>About</a> ||| <a href=#publications>Publications</a> </p>
</div>

<h1 id="about">Kfir Aberman</h1>
<div id="container">
 <div class="container__text">
	 I'm a Research Scientist and a team lead at <a href="https://research.snap.com/">Snap Research</a>.<br/>
	 My passion is synthesizing novel visual effects that are impactful and meaningful to people’s day to day.
	 I conduct research for various computer graphics applications using the power of deep neural networks, and develop the next generation of cool, creative, generative features for Snap.
	 <br/><br>


	 Prior to Snap I was a Reserch Scientist and manager at <a href="https://research.google/">Google Research</a>. I received my Ph.D from Tel-Aviv University where I was advised by <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>.
	 I grew up in a small town in Israel called Zefat, lived in Tel-Aviv as an adult, spent 3 wonderful years in China as a researcher, and currently I’m based in San-Francisco.<br/><br>

	 I love to host at Snap (Palo Alto) students that are passionate about synthesis and visual effects, and I can host through the entire year!
	 If you would like to work with me, please reach out.<br/><br>
  Email: kfiraberman@gmail.com / <a href="https://scholar.google.com/citations?user=jdbZDakAAAAJ&hl=en">Scholar</a> / <a href="https://mobile.twitter.com/AbermanKfir">Twitter</a> <br/>
 </div>
 <img class="container__image" src="./images/kfir.JPG">
</div>

<!-- <h1 id="news">News</h1>
<div id="paper">
	<ul>
		<li>CASA 2023 - Technical Papers Committee</li>
		<li>SCA 2023 - Technical Papers Committee</li>
		<li>SIGGRAPH Asia 2022 - Technical Papers Committee</li>
	</ul>
</div> -->

<!-- <li>Talk at the Computational Photography Workshop</li> -->

<!-- <h1 id="publications">Shipped Features</h1>
<div id="paper">
	<ul>
		<li>Magic Eraser - Pixel 6</li>
	</ul>
</div> -->

<h1 id="students">Former Student Collaborators and Interns</h1>
<div id="paper">
	<ul>
		<li>Peizhuo Li</li>
		<li>Yijia Weng</li>
		<li>Mingyi Shi</li>
		<li>Yotam Nitzan</li>
		<li>Sigal Raab</li>
		<li>Nataniel Ruiz</li>
		<li>Amir Hertz</li>
		<li>Ron Mokady</li>
		<li>Omri Avrahami</li>
		<li>Zihan Zhang</li>
	</ul>
</div>


<h1 id="publications">Publications</h1>
<div id="paper">
	<img class="paper_image" src="./images/HDB.jpg">
  <div class="paper_text">
  <strong>HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models</strong> <br/>
  Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, <strong>Kfir Aberman</strong> <br/>
  <i>Arxiv 2023</i> <br/>
  <a href="https://arxiv.org/abs/2307.06949"><code>Paper</code></a>
  <a href="https://hyperdreambooth.github.io/"><code>Project</code></a>
  </div>
  <br/><br/><br>
	<img class="paper_image" src="./images/B-A-S.gif">
	<div class="paper_text">
	<strong>Break-A-Scene: Extracting Multiple Concepts from a Single Image</strong> <br/>
	Omri Avrahami, <strong>Kfir Aberman</strong>, Ohad Fried, Daniel Cohen-Or, Dani Lischinski<br/>
	<i>ArXiv, 2023</i> <br/>
	<a href="https://arxiv.org/abs/2305.16311"><code>Paper</code></a>
	<a href="https://omriavrahami.com/break-a-scene/"><code>Project</code></a>
	<code>Code (coming soon)</code>
	</div>
	<br/><br/><br>
	<img class="paper_image" src="./images/DDS.jpg">
	<div class="paper_text">
	<strong>Delta Denoising Score</strong> <br/>
	Amir Hertz, <strong>Kfir Aberman</strong>, Daniel Cohen-Or<br/>
	<i>ICCV 2023</i> <br/>
	<a href="https://arxiv.org/abs/2304.07090"><code>Paper</code></a>
	<a href="https://delta-denoising-score.github.io/"><code>Project</code></a>
	<code>Code (coming soon)</code>
	</div>
	<br/><br/><br>
	<img class="paper_image" src="./images/pplus.jpg">
	<div class="paper_text">
	<strong>P+: Extended Textual Conditioning in Text-to-Image Generation</strong> <br/>
	Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, <strong>Kfir Aberman</strong>
	<br/>
	<i>ArXiv, 2023</i> <br/>
	<a href="https://arxiv.org/abs/2303.09522"><code>Paper</code></a>
	<a href="https://prompt-plus.github.io/"><code>Project</code></a>
	<code>Code (coming soon)</code>
	</div>
	<br/><br/><br>
	<img class="paper_image" src="./images/DB3D.gif">
	<div class="paper_text">
	<strong>DreamBooth3D</strong> <br/>
	Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada,  <strong>Kfir Aberman</strong>, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani<br/>
	<i>ICCV 2023</i> <br/>
	<a href="https://arxiv.org/abs/2303.13508"><code>Paper</code></a>
	<a href="https://dreambooth3d.github.io/"><code>Project</code></a>
	</div>
	<br/><br/><br>
	<img class="paper_image" src="./images/SketchGuided.jpg">
	<div class="paper_text">
	<strong>Sketch-Guided Text-to-Image Diffusion Models</strong> <br/>
	Andrey Voynov, <strong>Kfir Aberman</strong>, Daniel Cohen-Or <br/>
	<i>SIGGRAPH 2023</i> <br/>
	<a href="https://arxiv.org/abs/2211.13752"><code>Paper</code></a>
	<a href="https://sketch-guided-diffusion.github.io/"><code>Project</code></a>
	<code>Code (coming soon)</code>
	</div>
	<br/><br/><br>
	<img class="paper_image" src="./images/NullText.jpg">
  <div class="paper_text">
  <strong>Null-Text Inversion for Editing Real Images using Guided Diffusion Models</strong> <br/>
  Ron Mokady*, Amir Hertz*, <strong>Kfir Aberman</strong>, Yael Pritch, Daniel Cohen-Or <br/>
  <i>CVPR 2023</i> <br/>
  <a href="https://arxiv.org/abs/2211.09794"><code>Paper</code></a>
  <a href="https://null-text-inversion.github.io/"><code>Project</code></a>
  <a href="https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images"><code>Code</code></a>
  </div>
  <br/><br/><br>
 <img class="paper_image" src="./images/DreamBooth.jpg">
 <div class="paper_text">
 <strong>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</strong> <br/>
 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, <strong>Kfir Aberman</strong> <br/>
 <i>CVPR 2023 (Best Paper Candidate)</i> <br/>
 <a href="https://arxiv.org/abs/2208.12242"><code>Paper</code></a>
 <a href="https://dreambooth.github.io/"><code>Project</code></a>
 <a href="https://github.com/google/DreamBooth/"><code>Dataset</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/prompt-to-prompt.jpg">
 <div class="paper_text">
 <strong>Prompt-to-Prompt Image Editing with Cross Attention Control</strong> <br/>
 Amir Hertz, Ron Mokady, Jay Tenenbaum, <strong>Kfir Aberman</strong>, Yael Pritch, Daniel Cohen-Or <br/>
 <i>ICLR 2023</i> <br/>
 <a href="https://arxiv.org/abs/2208.01626"><code>Paper</code></a>
 <a href="https://prompt-to-prompt.github.io/"><code>Project</code></a>
 <a href="https://github.com/google/prompt-to-prompt/"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/MyStyle.jpg">
 <div class="paper_text">
 <strong>MyStyle: A Personalized Generative Prior</strong> <br/>
Yotam Nitzan, <strong>Kfir Aberman</strong>, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen-or <br/>
<i>SIGGRAPH Asia 2022 (journal track)</i><br/>
 <a href="https://arxiv.org/abs/2203.17272"><code>Paper</code></a>
 <a href="https://mystyle-personalized-prior.github.io/"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=axWo_9Gt47o"><code>Video</code></a>
 <a href="https://github.com/google/mystyle"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/GANimator.jpg">
 <div class="paper_text">
 <strong>GANimator: Neural Motion Synthesis from a Single Sequence</strong> <br/>
 Peizhuo Li, <strong>Kfir Aberman</strong>, Zihan Zhang, Rana Hanocka, Olga Sorkine-Hornung<br/>
 <i>SIGGRAPH 2022 (journal track)</i><br/>
 <a href="https://arxiv.org/abs/2205.02625"><code>Paper</code></a>
 <a href="https://peizhuoli.github.io/ganimator/"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=OV9VoHMEeyI"><code>Video</code></a>
 <a href="https://github.com/PeizhuoLi/ganimator"><code>Code</code></a>
</div>
 <br/><br/><br>
 <img class="paper_image" src="./images/MoDi.jpg">
 <div class="paper_text">
 <strong>MoDi: Unconditional Motion Synthesis from Diverse Data</strong> <br/>
Sigal Raab, Inbal Leibovitch, Peizhuo Li, <strong>Kfir Aberman</strong>, Olga Sorkine-Hornung, Daniel Cohen-Or<br/>
<i>CVPR 2023</i> <br/>
 <a href="https://arxiv.org/abs/2206.08010"><code>Paper</code></a>
 <a href="https://sigal-raab.github.io/MoDi.html"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=IIog1sKpogc"><code>Video</code></a>
 <a href="https://github.com/sigal-raab/MoDi"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/DSP.jpg">
 <div class="paper_text">
 <strong>Deep Saliency Prior for Reducing Visual Distraction</strong> <br/>
 <strong>Kfir Aberman</strong>*, Junfeng He*, Yossi Gandelsman, Inbar Mosseri, David E. Jacobs, Kai Kohlhoff, Yael Pritch, Michael Rubinstein <br/>
 <i>CVPR 2022</i> <br/>
 <a href="https://deep-saliency-prior.github.io/saliency_driven_editing_preprint.pdf"><code>Paper</code></a>
 <a href="https://deep-saliency-prior.github.io/"><code>Project</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/RhythmDancer.gif">
 <div class="paper_text">
 <strong>Rhythm is a Dancer: Music-Driven Motion Synthesis with Global Structure</strong> <br/>
 Andreas Aristidou, Anastasios Yiannakidis, <strong>Kfir Aberman</strong>, Daniel Cohen-Or, Ariel Shamir, Yiorgos Chrysanthou <br/>
 <i>IEEE TVCG 2021</i><br/>
 <a href="https://arxiv.org/abs/2111.12159"><code>Paper</code></a>
 <a href="https://www.youtube.com/watch?v=oo8N2yfmQQQ"><code>Video</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/Neural_Blend_Shapes.jpg">
 <div class="paper_text">
 <strong>Learning Skeletal Articulations with Neural Blend Shapes</strong> <br/>
 Peizhuo Li, <strong>Kfir Aberman</strong>, Rana Hanocka, Libin Liu, Olga Sorkine-Hornung, Baoquan Chen<br/>
 <i>SIGGRAPH 2021</i><br/>
 <a href="https://arxiv.org/abs/2105.02451"><code>Paper</code></a>
 <a href="https://peizhuoli.github.io/neural-blend-shapes/"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=antc20EFh6k"><code>Video</code></a>
 <a href="https://github.com/PeizhuoLi/neural-blend-shapes"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/MotioNet.jpg">
 <div class="paper_text">
 <strong>MotioNet: 3D Human Motion Reconstruction from Video with Skeleton Consistency</strong><br/>
 Mingyi Shi, <strong>Kfir Aberman</strong>, Andreas Aristidou, Taku Komura, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen<br/>
 <i>Transactions on Graphics (ToG) 2020</i><br/>
 <a href="https://arxiv.org/abs/2006.12075"><code>Paper</code></a>
 <a href="https://rubbly.cn/publications/motioNet/"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=8YubchlzvFA"><code>Video</code></a>
 <a href="https://github.com/Shimingyi/MotioNet"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/SkeletonAware.gif">
 <div class="paper_text">
 <strong>Skeleton-Aware Networks for Deep Motion Retargeting</strong> <br/>
 <strong>Kfir Aberman</strong>*, Peizhuo Li*, Dani Lischinski, Olga Sorkine-Hornung, Daniel Cohen-Or, Baoquan Chen <br/>
 <i>SIGGRAPH 2020</i><br/>
 <a href="https://arxiv.org/abs/2005.05732"><code>Paper</code></a>
 <a href="https://deepmotionediting.github.io/retargeting"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=ym8Tnmiz5N8"><code>Video</code></a>
 <a href="https://github.com/DeepMotionEditing/deep-motion-editing"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/MotionStyleTransfer.gif">
 <div class="paper_text">
 <strong>Unpaired Motion Style Transfer from Video to Animation</strong> <br/>
 <strong>Kfir Aberman</strong>*, Yijia Weng*, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen<br/>
 <i>SIGGRAPH 2020</i><br/>
 <a href="https://arxiv.org/abs/2005.05751"><code>Paper</code></a>
 <a href="https://deepmotionediting.github.io/style_transfer"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=m04zuBSdGrc"><code>Video</code></a>
 <a href="https://github.com/DeepMotionEditing/deep-motion-editing"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/2DRetargeting.gif">
 <div class="paper_text">
 <strong>Learning Character-Agnostic Motion for Motion Retargeting in 2D</strong> <br/>
 <strong>Kfir Aberman</strong>, Rundi Wu, Dani Lischinski, Chen Baoquan, Daniel Cohen-Or <br/>
 <i>SIGGRAPH 2019</i><br/>
 <a href="https://arxiv.org/abs/1905.01680"><code>Paper</code></a>
 <a href="http://motionretargeting2d.github.io"><code>Project</code></a>
 <a href="https://youtu.be/fR4h4OjZSdU"><code>Video</code></a>
 <a href="https://github.com/ChrisWu1997/2D-Motion-Retargeting"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/DVPC.jpg">
 <div class="paper_text">
 <strong>Deep Video-Based Performance Cloning</strong> <br/>
 <strong>Kfir Aberman</strong>, Mingyi Shi, Jing Liao, Dani Lischinski, Chen Baoquan, Daniel Cohen-Or <br/>
 <i>Eurographics 2019</i><br/>
 <a href="https://arxiv.org/abs/1808.06847"><code>Paper</code></a>
 <a href="https://www.youtube.com/watch?v=JpwsEeqNhhA"><code>Video</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/NBBs.jpg">
 <div class="paper_text">
 <strong>Neural Best-Buddies: Sparse Cross-Domain Correspondence</strong> <br/>
 <strong>Kfir Aberman</strong>, Jing Liao, Mingyi Shi, Dani Lischinski, Chen Baoquan, Daniel Cohen-Or <br/>
 <i>SIGGRAPH 2018</i><br/>
 <a href="https://arxiv.org/abs/1805.04140"><code>Paper</code></a>
 <a href="https://kfiraberman.github.io/neural_best_buddies/"><code>Project</code></a>
 <a href="https://www.youtube.com/watch?v=tYqkMGaGmkk&t=83s"><code>Video</code></a>
 <a href="https://github.com/kfiraberman/neural_best_buddies"><code>Code</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/DipTransform.jpg">
 <div class="paper_text">
 <strong>Dip Transform for 3D Shape Reconstruction</strong> <br/>
 <strong>Kfir Aberman</strong>, Oren Katzir, Qiang Zhou, Zegang Luo, Andrei Sharf, Chen Greif, Chen Baoquan, Daniel Cohen-Or<br/>
 <i>SIGGRAPH 2017</i><br/>
 <a href="https://www.cs.tau.ac.il/~dcor/articles/2017/Dip.Transform.pdf"><code>Paper</code></a>
 </div>
 <br/><br/><br>
 <img class="paper_image" src="./images/SubNyquistSAR.jpg">
 <div class="paper_text">
 <strong>Sub-Nyquist SAR via Fourier Domain Range-Doppler Processing</strong> <br/>
 <strong>Kfir Aberman</strong>, Yonina C. Eldar<br/>
 <i>IEEE Transactions on Geoscience and Remote Sensing 2017</i><br/>
 <a href="https://arxiv.org/pdf/1608.04138"><code>Paper</code></a>
 </div>
 <br/><br/><br>
</body>
</html>
